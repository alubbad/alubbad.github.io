<head>
    <title>Project 5: Fun With Diffusion Models!</title>
    <link rel="stylesheet" href="styles.css" />
</head>

<body>
    <h1>CS180 Project 5: Fun With Diffusion Models!</h1>
    <div class="container">
    <h2>Part A: The Power of Diffusion Models!</h2>

    <h3>Part 0: Setup</h3>
    <p>I created a dictionary of embeddings and chose 3 text prompts to generate imgs. The num_inference_steps values I chose 
        are 10 and 100. I noticed that the higher num_inference_steps values result in more detailed imgs with better
        quality.
    </p>
    <p>I'm using a random seed of value 100.</p>
    <h4>num_inference_steps=10</h4>
    <div class="row">
        <figure><img src="media/snow10.png" ></img>
        <figcaption style="font-weight:bold">an oil painting of a snowy mountain village</figcaption></figure>
        <figure><img src="media/amalfi10.png"></img>
        <figcaption style="font-weight:bold">a photo of the amalfi coast</figcaption></figure>
        <figure><img src="media/dog10.png"></img>
        <figcaption style="font-weight:bold">a photo of a dog</figcaption></figure>
    </div>
    <h4>num_inference_steps=100</h4>
    <div class="row">
        <figure><img src="media/snow100.png" ></img>
        <figcaption style="font-weight:bold">an oil painting of a snowy mountain village</figcaption></figure>
        <figure><img src="media/amalfi100.png"></img>
        <figcaption style="font-weight:bold">a photo of the amalfi coast</figcaption></figure>
        <figure><img src="media/dog100.png"></img>
        <figcaption style="font-weight:bold">a photo of a dog</figcaption></figure>
    </div>
    </div>

    <div class="container">
    <h3>Part 1: Sampling Loops</h2>
    <h3>1.1 Implementing the Forward Process</h3>
    <p> I implemented the forward process on an image of the campanile, which takes a clean image and adds noise to it.
    </p>
    <div class="row">
        <figure><img src="media/campanile_original.png"></img>
        <figcaption style="font-weight:bold">Original Image</figcaption></figure>
        <figure><img src="media/campanile_noise_250.png"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=250</figcaption></figure>
        <figure><img src="media/campanile_noise_500.png"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=500</figcaption></figure>
        <figure><img src="media/campanile_noise_750.png"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=750</figcaption></figure>
    </div>
    <h3>1.2 Classical Denoising</h3>
    <p> I took the noisy images for timesteps [250, 500, 750], and used Gaussian blur filtering to try to remove the noise. The 
        results were not good and did not manage to denoise properly. 
    </p>
    <div class="row">
        <figure><img src="media/campanile_noise_250.png" style="width:300px"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=250</figcaption></figure>
        <figure><img src="media/campanile_noise_500.png" style="width:300px"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=500</figcaption></figure>
        <figure><img src="media/campanile_noise_750.png" style="width:300px"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=750</figcaption></figure>
    </div>
        <div class="row">
        <figure><img src="media/campanile_denoise_250.png" style="width:300px"></img>
        <figcaption style="font-weight:bold">Gaussian Blur Denoising at t=250</figcaption></figure>
        <figure><img src="media/campanile_denoise_500.png" style="width:300px"></img>
        <figcaption style="font-weight:bold">Gaussian Blur Denoising at t=500</figcaption></figure>
        <figure><img src="media/campanile_denoise_750.png" style="width:300px"></img>
        <figcaption style="font-weight:bold">Gaussian Blur Denoising at t=750</figcaption></figure>
    </div>
    <h3>1.3 One-Step Denoising</h3>
    <p>I used a pretrained UNet to perform one step denoising on the noisy images. It was used to recover Gaussian noise 
        from the image. Then, I removed this noise to recover the original image.
    </p>
    <div class="row">
    <figure><img src="media/campanile_original.png" style="width:300px"></img>
    <figcaption style="font-weight:bold">Original Campanile</figcaption></figure>
    <figure><img src="media/campanile_noise_250.png" style="width:300px"></img>
    <figcaption style="font-weight:bold">Noisy Image at t=250</figcaption></figure>
    <figure><img src="media/campanile_250.png" style="width:300px"></img>
    <figcaption style="font-weight:bold">One Step Denoising at t=250</figcaption></figure>
    </div>
    <div class="row">
    <figure><img src="media/campanile_original.png" style="width:300px"></img>
    <figcaption style="font-weight:bold">Original Campanile</figcaption></figure>
    <figure><img src="media/campanile_noise_500.png" style="width:300px"></img>
    <figcaption style="font-weight:bold">Noisy Image at t=500</figcaption></figure>
    <figure><img src="media/campanile_500.png" style="width:300px"></img>
    <figcaption style="font-weight:bold">One Step Denoising at t=500</figcaption></figure>
    </div>
    <div class="row">
    <figure><img src="media/campanile_original.png" style="width:300px"></img>
    <figcaption style="font-weight:bold">Original Campanile</figcaption></figure>
    <figure><img src="media/campanile_noise_750.png" style="width:300px"></img>
    <figcaption style="font-weight:bold">Noisy Image at t=750</figcaption></figure>
    <figure><img src="media/campanile_750.png" style="width:300px"></img>
    <figcaption style="font-weight:bold">One Step Denoising at t=750</figcaption></figure>
    </div>
    <h3>1.4 Iterative Denoising</h3>
    <p>
        I implemented iterative denoising by starting with a noisy image at t=990 and progressively reduced noise using strided 
        timesteps.
    </p>
    <div class="row">
        <figure><img src="media/noisy_campanile_90.png" style="width:250px"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=90</figcaption></figure>
        <figure><img src="media/noisy_campanile_240.png" style="width:250px"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=240</figcaption></figure>
        <figure><img src="media/noisy_campanile_390.png" style="width:250px"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=390</figcaption></figure>
        <figure><img src="media/noisy_campanile_540.png" style="width:250px"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=540</figcaption></figure>
        <figure><img src="media/noisy_campanile_690.png" style="width:250px"></img>
        <figcaption style="font-weight:bold">Noisy Campanile at t=690</figcaption></figure>
    </div>
        <div class="row">
        <figure><img src="media/campanile_1.4.png" style="width:1200px"></img></figure>
    </div>
    <h3>1.5 Diffusion Model Sampling</h3>
    <p>
        In this part, a diffusion model was used to generate images from scratch. We do this by setting i_start = 0 and passing 
        im_noisy as random noise. This effectively denoises pure noise. I used the prompt "a high quality photo"
    </p>
    <div class="row">
        <figure><img src="media/images_1.5.png" style="width:1200px"></img></figure>
    </div>

    <h3>1.6 Classifier-Free Guidance (CFG)</h3>
    <p>
        I used classifier free guidance to enhance the quality of the generated images by combining conditional and unconditional
        noise estimates with a scaling factor set to 7.
    </p>
    <div class="row">
        <figure><img src="media/images_1.6.png" style="width:1200px"></img></figure>
    </div>

    <h3>1.7 Image-to-image Translation</h3>
    <div class="row">
        <figure><img src="media/images_1.7.png" style="width:500px"></img></figure>
        <figure><img src="media/images_1.7(2).png" style="width:500px"></img></figure>
        <figure><img src="media/campanile_original.png" style="width:180px"></img>
        <figcaption style="font-weight:bold">Original Campanile</figcaption></figure>
    </div>
        <div class="row">
        <figure><img src="media/suitcase_1.7.png" style="width:500px"></img></figure>
        <figure><img src="media/suitcase_1.7(2).png" style="width:500px"></img></figure>
        <figure><img src="media/suitcase_original.png" style="width:180px"></img>
        <figcaption style="font-weight:bold">Original Suitcase</figcaption></figure>
    </div>
    <div class="row">
        <figure><img src="media/chair_1.7.png" style="width:500px"></img></figure>
        <figure><img src="media/chair_1.7(2).png" style="width:500px"></img></figure>
        <figure><img src="media/chair_original.png" style="width:180px"></img>
        <figcaption style="font-weight:bold">Original Chair</figcaption></figure>
    </div>
    <h3>1.7.1 Editing Hand-Drawn and Web Images</h3>
    <h4>Results: Web Image</h4>
    <div class="row">
        <figure>
            <img src="media/watermelon_gen.png" style="width:1000px; height:200px"></img>
            <figcaption>Generated Images</figcaption>
        </figure>
        <figure>
            <img src="media/watermelon_og.png" style="width:180px; height:200px"></img>
            <figcaption>Original Image</figcaption>
        </figure>
    </div>
    <h4>Results: Hand Drawn Images</h4>
    <div class="row">
        <figure>
            <img src="media/lake_gen.png" style="width:1000px; height:200px"></img>
            <figcaption>Generated Images</figcaption>
        </figure>
        <figure>
            <img src="media/lake_og.png" style="width:180px; height:200px"></img>
            <figcaption>Original Image</figcaption>
        </figure>
    </div>
    <h3>1.7.2 Inpainting</h3>
    <p>
        Here I used a mask to apply iterative denoising and generate new content in the masked areas. With this method,
        I inpainted the top of the campanile and the Palace of Fine Arts as well as the Mona Lisa's head.
    </p>
    <div class="row">
        <figure>
            <img src="media/campanile_original.png"></img>
            <figcaption>Campanile</figcaption>
        </figure>
        <figure>
            <img src="media/mask.png"></img>
            <figcaption>Mask</figcaption>
        </figure>
        <figure>
            <img src="media/replace.png"></img>
            <figcaption>Hole to Fill</figcaption>
        </figure>
        <figure>
            <img src="media/campanile_inpainted.png"></img>
            <figcaption>Campanile Inpainted</figcaption>
        </figure>
    </div>
    <div class="row">
        <figure>
            <img src="media/palace.png"></img>
            <figcaption>Campanile</figcaption>
        </figure>
        <figure>
            <img src="media/mask.png"></img>
            <figcaption>Mask</figcaption>
        </figure>
        <figure>
            <img src="media/replace_2.png"></img>
            <figcaption>Hole to Fill</figcaption>
        </figure>
        <figure>
            <img src="media/palace_inpainted.png"></img>
            <figcaption>Campanile Inpainted</figcaption>
        </figure>
    </div>
        <div class="row">
        <figure>
            <img src="media/mona_lisa.png"></img>
            <figcaption>Campanile</figcaption>
        </figure>
        <figure>
            <img src="media/mask_2.png"></img>
            <figcaption>Mask</figcaption>
        </figure>
        <figure>
            <img src="media/replace_3.png"></img>
            <figcaption>Hole to Fill</figcaption>
        </figure>
        <figure>
            <img src="media/mona_lisa_inpainted.png"></img>
            <figcaption>Campanile Inpainted</figcaption>
        </figure>
    </div>
    <h3>Text-Conditioned Image-to-image Translation</h3>
    <p>
        In this part, I used the prompt "a pencil" to guide how an image evolves during denoising. The process blends the original 
        image with pencil-themed elements. I applied this on the campanile image and two of my own images as well! The images on the
        far right of each row are the original images.
    </p>
    <div class="row">
        <figure>
            <img src="media/campanile_pencil.png" style="width: 760px"></img>
        </figure>
        <figure>
            <img src="media/campanile_pencil(2).png" style="width: 370px"></img>
        </figure>
        <figure>
            <img src="media/campanile_original.png" style="width: 200px"></img>
        </figure>
    </div>
        <div class="row">
        <figure>
            <img src="media/palace_pencil.png" style="width: 550px"></img>
        </figure>
        <figure>
            <img src="media/palace_pencil(2).png" style="width: 550px"></img>
        </figure>
        <figure>
            <img src="media/palace.png" style="width: 200px"></img>
        </figure>
    </div>
    <div class="row">
        <figure>
            <img src="media/suitcase_pencil.png" style="width: 550px"></img>
        </figure>
        <figure>
            <img src="media/suitcase_pencil(2).png" style="width: 550px"></img>
        </figure>
        <figure>
            <img src="media/suitcase_original.png" style="width: 200px"></img>
        </figure>
    </div>
    <h3>1.8 Visual Anagrams</h3>
    <p>
        In this part, I implemented Visual Anagrams and created optical illusions with diffusion models where
        an image that looks like "an oil painting of people around a campfire", but when flipped upside down 
        will reveal "an oil painting of an old man". By denoising the image with two different prompts (one for original and one
        for flipped) and averaging the noise estimates, we get this effect.
    </p>
    <div class="row">
        <figure>
            <img src="media/old_man.png"></img>
            <figcaption>"an oil painting of an old man"</figcaption>
        </figure>
        <figure>
            <img src="media/campfire_1.png"></img>
            <figcaption>"an oil painting of people around a campfire"</figcaption>
        </figure>
    </div>

    <div class="row">
        <figure>
            <img src="media/barista.png"></img>
            <figcaption>"a photo of a hipster barista"</figcaption>
        </figure>
        <figure>
            <img src="media/hat-man.png"></img>
            <figcaption>"a man wearing a hat"</figcaption>
        </figure>
    </div>
    <h3>1.9 Hybrid Images</h3>
    <p>
        For hybrid images, we combine low-frequency details from one image with high-frequency details from another using a 
        diffusion model. We apply high-pass and low-pass filters on the noise estimates for the different prompts.
    </p>
    <div class="row">
        <figure>
            <img src="media/skull-waterfall.png"></img>
            <figcaption>Far:"a lithograph of a skull". Close: "a lithograph of waterfalls"</figcaption>
        </figure>
        <figure>
            <img src="media/amalfi_hat(1).png"></img>
            <figcaption>Far:"a man wearing a hat". Close: "a photo of the amalfi coast"</figcaption>
        </figure>
    </div>
</div>

<div class="container">
    <h2>Part A: The Power of Diffusion Models!</h2>

    <h3>Part 1: Training a Single-Step Denoising UNet</h3>
    <p>In this part, I built a simple one-step denoiser where given a noisy image z, I trained a denoiser D such that
        it maps z to a clean image x.
    </p>
    <h3>1.1 Implementing the UNet</h3>
    <p>Below is the diagram for the unconditional UNet I built for the denoiser.</p>
     <figure>
            <img src="media/1.1_unet.png" style="width:500px"></img>
    </figure>

    <h3>1.2 Using the UNet to Train a Denoiser</h3>
    <h4>Noising Process Visualization</h4>
    <p>Below I show the noising process using sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]</p>
    <figure>
            <img src="media/5b_1.2.png" style="width:600px"></img>
    </figure>

    <h4>Training Results</h4>
    <p>I trained with sigma = 0.5, batch size 256, over 5 epochs, and learning rate 1e-4.</p>
    <div class="row">
        <figure>
            <img src="media/1.2.1_training_loss_curve.png" style="height:400px; width:auto"></img>
        </figure>
        <figure>
            <img src="media/1.2.1_average_loss_epoch.png" style="height:400px; width:auto"></img>
        </figure>
    </div>
    <div class="row">
        <figure>
            <img src="media/1.2.1_epoch1.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 1 epoch of training</figcaption>
        </figure>
        <figure>
            <img src="media/1.2.1_epoch5.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 5 epochs of training</figcaption>
        </figure>
    </div>
    <h4>Out of Distribution Testing</h4>
    <p>I tested to see how my denoiser works with other noise levels. I observed that although it works great for sigma = 0.5, 
        the model's performance drops significantly for very very high noise levels. Also, for noise level 0, it results in a
        blurrier image because it tries to denoise an clean denoised image.
    </p>
    <div class="row">
        <figure>
            <img src="media/1.2.2.png" style="width:400px"></img>
            <figcaption>Denoising for different noise levels</figcaption>
        </figure>
    </div>
    
    <h4>Denoising Pure Noise Results</h4>
    <p>Below are the results from denoising a pure, random Gaussian noise.</p>
        <div class="row">
        <figure>
            <img src="media/1.2.3_train_curve.png" style="height:400px; width:auto"></img>
        </figure>
        <figure>
            <img src="media/1.2.3_epoch_curve.png" style="height:400px; width:auto"></img>
        </figure>
    </div>
    <div class="row">
        <figure>
            <img src="media/1.2.3_epoch1.png" style="width:500px"></img>
            <figcaption>Results on digits from the test set after 1 epoch of training</figcaption>
        </figure>
        <figure>
            <img src="media/1.2.3_epoch5.png" style="width:600px"></img>
            <figcaption>Results on digits from the test set after 5 epochs of training</figcaption>
        </figure>
    </div>
    <h4>Observations</h4>
    <p>The model generates the same blurry image reardless of input noise. This is because it minimizes the expected square 
        differences to all training examples. This means that since pure noise has no structure and contains no information
        about which digit to generate, the optimal prediction will be the mean of the training distribution. This is why the 
        result is the same blurry images containing features from multiple digits but not representing any single digit clearly.
    </p>
</div>
<div class="container">
    <h2>Part 2: Training a Flow Matching Model</h2>

    <h3>2.1 Adding Time Conditioning to UNet</h3>
    <p>I injected a scalar t into my UNet model to condition it. </p>
    <div class="row">
        <figure>
            <img src="media/2.1_unet.png" style="width:500px"></img>
        </figure>
    </div>

    <h3>2.2 Training the UNet</h3>
    <p>To train the time-conditioned UNet, we pick a random image x from the training set, set a random timestep t, add noise to
        x to get xt, and train the denoiser to predict the flow at xt.
    </p>
    <h4>Training loss curves</h4>
    <div class="row">
        <figure>
            <img src="media/2.2_train_curve.png" style="height:400px; width:auto"></img>
        </figure>
        <figure>
            <img src="media/2.2_epoch_curve.png" style="height:400px; width:auto"></img>
        </figure>
    </div>

    <h3>2.3 Sampling from the UNet</h3>
    <p>Sample quality improves dramatically with more training. After 1 epoch, there are some recognizable features but
        it's still hard to recognize the digit. By epoch 10, the generated digits are much more clear and recognizable.</p>
    <div class="row">
        <figure>
            <img src="media/2.3_epoch1.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 1 epoch of training</figcaption>
        </figure>
        <figure>
            <img src="media/2.3_epoch5.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 5 epochs of training</figcaption>
        </figure>
        <figure>
            <img src="media/2.3_epoch10.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 10 epochs of training</figcaption>
        </figure>
    </div>
    <div class="row">
        <figure>
            <img src="media/2.3_comparison.png" style="width:500px"></img>
        </figure>
    </div>

    <h3>2.4 Adding Class-Conditioning to UNet</h3>
    <p>To improve our image generation, we conditioned our UNet on the class of the digit 0-9.</p>

    <h3>2.5 Training the UNet</h3>
    <h4>Training loss curves for class-conditional flow matching model with classifier-free guidance</h4>
    <div class="row">
        <figure>
            <img src="media/2.5_train_curve.png" style="height:400px; width:auto"></img>
        </figure>
        <figure>
            <img src="media/2.5_epoch_curve.png" style="height:400px; width:auto"></img>
        </figure>
    </div>

    <h3>2.6 Sampling from the UNet</h3>
    <p>Showing 4 instances of each digit at different training stages with exponential learning rate scheduler:</p>
    <div class="row">
        <figure>
            <img src="media/2.6_epoch1.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 1 epoch of training</figcaption>
        </figure>
        <figure>
            <img src="media/2.6_epoch5.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 5 epochs of training</figcaption>
        </figure>
        <figure>
            <img src="media/2.6_epoch10.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 10 epochs of training</figcaption>
        </figure>
    </div>
    <div class="row">
        <figure>
            <img src="media/2.6_comparison.png" style="width:500px"></img>
        </figure>
    </div>
    <p>We can see that our results adding class conditioning here are much better than just time conditioning after 10 epochs.</p>

    <h3>Fixed Learning Rate (No Scheduler)</h3>
    <p>I removed the learning rate scheduler and decreased the learning rate to 1e-3. This ended up improving the results for epoch 
        1 but it still converged to almost the same quality of images as the previous denoiser for epoch 10.
    </p>
    <div class="row">
        <figure>
            <img src="media/2.6.2_epoch1.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 1 epoch of training</figcaption>
        </figure>
        <figure>
            <img src="media/2.6.2_epoch5.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 5 epochs of training</figcaption>
        </figure>
        <figure>
            <img src="media/2.6.2_epoch10.png" style="width:400px"></img>
            <figcaption>Results on digits from the test set after 10 epochs of training</figcaption>
        </figure>
    </div>
    <div class="row">
        <figure>
            <img src="media/2.6.2_comparison.png" style="width:500px"></img>
        </figure>
    </div>
</div>
</body>